{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37e2724a",
   "metadata": {},
   "source": [
    "# Citation Sentiment Pipeline (With Transformers)\n",
    "\n",
    "This notebook extends the **Basic** pipeline by adding two ML models:\n",
    "\n",
    "- **MultiCite (AllenAI)** `allenai/multicite-multilabel-scibert` → *multi-label citation intent*\n",
    "  - labels: `motivation`, `background`, `uses`, `extends`, `similarities`, `differences`, `future_work`\n",
    "  - we report the **top-1 label** and probability, and all labels above a threshold (default 0.5)\n",
    "- **SiEBERT (RoBERTa-large)** `siebert/sentiment-roberta-large-english` → *binary sentiment*\n",
    "  - we report the **positive probability** (0–1) as `hf_sent_score`\n",
    "\n",
    "The rest of the pipeline is the same:\n",
    "1) `Cited by` → citing PMIDs, 2) citing abstracts → sentences, 3) compute indicators, 4) CSV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c16d42e",
   "metadata": {},
   "source": [
    "## Parameters (edit here and Run All)\n",
    "\n",
    "- `TARGET_PMID`: the PubMed ID of the paper you analyze\n",
    "- `OUT_DIR`: where to store CSV\n",
    "- `MAX_CITING`: (optional) limit the number of citing papers (None for all)\n",
    "- `NCBI_API_KEY`: (optional) set your key to raise rate limit from 3 to 10 req/s\n",
    "\n",
    "> Tip: Get a free NCBI API key at <https://www.ncbi.nlm.nih.gov/account/> and set it here or as an env var.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df29adbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> Edit here <<<\n",
    "TARGET_PMID = \"10519872\"  # Example: Re-emergent tremor of Parkinson's disease (JNNP, 1999)\n",
    "OUT_DIR = \"results_csv\"\n",
    "MAX_CITING = 50          # set to None for all\n",
    "NCBI_API_KEY = \"\"        # optional API key\n",
    "PAUSE = 0.25             # polite pause between API calls (seconds)\n",
    "\n",
    "# Transformers\n",
    "MULTICITE_MODEL = \"allenai/multicite-multilabel-scibert\"\n",
    "SIEBERT_MODEL   = \"siebert/sentiment-roberta-large-english\"\n",
    "MULTICITE_THRESH = 0.50  # labels above this probability will be included in mc_labels_multi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8048af",
   "metadata": {},
   "source": [
    "## Install & Data (one-time)\n",
    "\n",
    "Run this cell once per environment. This installs both the **basic** deps and **Transformers**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7394c018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# installs (skip if already installed)\n",
    "%pip -q install requests pandas tqdm nltk textblob lxml transformers torch sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4ff8331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/yutaashihara/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/yutaashihara/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/yutaashihara/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-time downloads for NLTK resources\n",
    "import nltk\n",
    "nltk.download('punkt')       # 既に入っていてもOK\n",
    "nltk.download('punkt_tab')   # ★ これが新たに必要\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafc01f1",
   "metadata": {},
   "source": [
    "## About API limits\n",
    "\n",
    "- PubMed E-utilities rate limit: **3 req/sec** (or **10 req/sec** with API key).\n",
    "- This notebook sleeps between calls to be polite. You can tune `PAUSE` if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518b0d8d",
   "metadata": {},
   "source": [
    "## Load libraries & define helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "259abeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/wholebif/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/wholebif/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 73\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# get label map from config (id2label)\u001b[39;00m\n\u001b[1;32m     72\u001b[0m mc_id2label \u001b[38;5;241m=\u001b[39m mc_mod\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mid2label\n\u001b[0;32m---> 73\u001b[0m mc_label_order \u001b[38;5;241m=\u001b[39m [mc_id2label[\u001b[38;5;28mstr\u001b[39m(i)] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mc_id2label, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m mc_id2label[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(mc_mod\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_labels)]\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmulticite_labels\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m, thresh: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "Cell \u001b[0;32mIn[4], line 73\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# get label map from config (id2label)\u001b[39;00m\n\u001b[1;32m     72\u001b[0m mc_id2label \u001b[38;5;241m=\u001b[39m mc_mod\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mid2label\n\u001b[0;32m---> 73\u001b[0m mc_label_order \u001b[38;5;241m=\u001b[39m [\u001b[43mmc_id2label\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mc_id2label, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m mc_id2label[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(mc_mod\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_labels)]\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmulticite_labels\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m, thresh: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mKeyError\u001b[0m: '0'"
     ]
    }
   ],
   "source": [
    "import os, time, re, math, requests, pandas as pd, torch, torch.nn.functional as F\n",
    "from lxml import etree\n",
    "from tqdm import tqdm\n",
    "from nltk import sent_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "EU = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
    "\n",
    "def _get(url, **params):\n",
    "    if NCBI_API_KEY:\n",
    "        params.setdefault(\"api_key\", NCBI_API_KEY)\n",
    "    r = requests.get(url, params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r\n",
    "\n",
    "def get_pubmed_meta(pmid: str):\n",
    "    xml = _get(EU+\"efetch.fcgi\", db=\"pubmed\", id=pmid, retmode=\"xml\").text\n",
    "    root = etree.fromstring(xml.encode())\n",
    "    title = root.xpath('string(.//ArticleTitle)') or \"\"\n",
    "    journal = root.xpath('string(.//Journal/Title)') or \"\"\n",
    "    pmcid = root.xpath('string(.//ArticleIdList/ArticleId[@IdType=\"pmc\"])') or \"\"\n",
    "    doi   = root.xpath('string(.//ArticleIdList/ArticleId[@IdType=\"doi\"])') or \"\"\n",
    "    return dict(title=title.strip(), journal=journal.strip(), pmcid=pmcid.strip(), doi=doi.strip())\n",
    "\n",
    "def get_citing_pmids(pmid: str):\n",
    "    j = _get(EU+\"elink.fcgi\", dbfrom=\"pubmed\", linkname=\"pubmed_pubmed_citedin\", id=pmid, retmode=\"json\").json()\n",
    "    dbs = j[\"linksets\"][0].get(\"linksetdbs\", [])\n",
    "    pmids = dbs[0][\"links\"] if dbs else []\n",
    "    return pmids\n",
    "\n",
    "def get_abstract_sentences(pmid: str):\n",
    "    xml = _get(EU+\"efetch.fcgi\", db=\"pubmed\", id=pmid, retmode=\"xml\").text\n",
    "    root = etree.fromstring(xml.encode())\n",
    "    abst = ' '.join(root.xpath('.//AbstractText/text()')).strip()\n",
    "    return sent_tokenize(abst) if abst else []\n",
    "\n",
    "# ---- Scorers: VADER / TextBlob / Custom ----\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "TOKEN = re.compile(r\"[A-Za-z']+\")\n",
    "CUSTOM_POS = {\"support\",\"increase\",\"enhance\",\"robust\",\"effective\",\"novel\",\"improve\",\"key\",\"helps\",\"correlated\",\"important\",\"useful\"}\n",
    "CUSTOM_NEG = {\"reduce\",\"decrease\",\"inhibit\",\"fail\",\"negative\",\"contradict\",\"weak\",\"poor\",\"limited\"}\n",
    "\n",
    "def vader01(text: str) -> float:\n",
    "    return (sid.polarity_scores(text)[\"compound\"] + 1) / 2\n",
    "\n",
    "def textblob01(text: str) -> float:\n",
    "    return (TextBlob(text).sentiment.polarity + 1) / 2\n",
    "\n",
    "def ratios_and_custom(text: str):\n",
    "    toks = [t.lower() for t in TOKEN.findall(text)]\n",
    "    if not toks:\n",
    "        return 0.5, 0.0, 0.0\n",
    "    S = set(toks)\n",
    "    pos = len(S & CUSTOM_POS)\n",
    "    neg = len(S & CUSTOM_NEG)\n",
    "    total = len(S)\n",
    "    pos_ratio = pos / total\n",
    "    neg_ratio = neg / total\n",
    "    score = (pos - neg) / (pos + neg + 1e-6)\n",
    "    score01 = (score + 1) / 2\n",
    "    return score01, pos_ratio, neg_ratio\n",
    "\n",
    "# ---- Transformers: MultiCite (multi-label) & SiEBERT (binary sentiment) ----\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "mc_tok = AutoTokenizer.from_pretrained(MULTICITE_MODEL)\n",
    "mc_mod = AutoModelForSequenceClassification.from_pretrained(MULTICITE_MODEL).to(device).eval()\n",
    "\n",
    "# get label map from config (id2label)\n",
    "mc_id2label = mc_mod.config.id2label\n",
    "mc_label_order = [mc_id2label[str(i)] if isinstance(mc_id2label, dict) else mc_id2label[i] for i in range(mc_mod.config.num_labels)]\n",
    "\n",
    "def multicite_labels(text: str, thresh: float = 0.5):\n",
    "    with torch.no_grad():\n",
    "        out = mc_mod(**mc_tok(text, return_tensors='pt', truncation=True, max_length=512).to(device))\n",
    "        # Multi-label → sigmoid\n",
    "        probs = torch.sigmoid(out.logits)[0].detach().cpu().numpy()\n",
    "    # top-1\n",
    "    top_idx = int(probs.argmax())\n",
    "    top_label = mc_label_order[top_idx]\n",
    "    top_prob = float(probs[top_idx])\n",
    "    # all labels above threshold\n",
    "    multi = [lbl for lbl, p in zip(mc_label_order, probs) if p >= thresh]\n",
    "    return top_label, top_prob, multi, probs.tolist()\n",
    "\n",
    "# SiEBERT (pos/neg)\n",
    "sb_tok = AutoTokenizer.from_pretrained(SIEBERT_MODEL)\n",
    "sb_mod = AutoModelForSequenceClassification.from_pretrained(SIEBERT_MODEL).to(device).eval()\n",
    "\n",
    "def hf_sentiment_posprob(text: str) -> float:\n",
    "    with torch.no_grad():\n",
    "        out = sb_mod(**sb_tok(text, return_tensors='pt', truncation=True, max_length=512).to(device))\n",
    "        probs = torch.softmax(out.logits, dim=-1)[0]\n",
    "        pos_prob = float(probs[1].detach().cpu())\n",
    "    return pos_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e862663e",
   "metadata": {},
   "source": [
    "## Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5220db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "meta = get_pubmed_meta(TARGET_PMID)\n",
    "src_title, src_journal = meta[\"title\"], meta[\"journal\"]\n",
    "\n",
    "citing = get_citing_pmids(TARGET_PMID)\n",
    "if MAX_CITING is not None:\n",
    "    citing = citing[:MAX_CITING]\n",
    "\n",
    "rows = []\n",
    "for cpmid in tqdm(citing, desc=f\"Cited-by for PMID {TARGET_PMID}\"):\n",
    "    try:\n",
    "        cmeta = get_pubmed_meta(cpmid)\n",
    "        ct_title = cmeta[\"title\"]\n",
    "        sentences = get_abstract_sentences(cpmid)\n",
    "        if not sentences:\n",
    "            continue\n",
    "        for sent in sentences:\n",
    "            # basic\n",
    "            v_vader = vader01(sent)\n",
    "            v_tb = textblob01(sent)\n",
    "            v_custom, r_pos, r_neg = ratios_and_custom(sent)\n",
    "            # transformers\n",
    "            mc_label, mc_prob, mc_multi, mc_all = multicite_labels(sent, thresh=MULTICITE_THRESH)\n",
    "            sb_pos = hf_sentiment_posprob(sent)\n",
    "            rows.append({\n",
    "                \"source_title\": src_title,\n",
    "                \"source_journal\": src_journal,\n",
    "                \"citing_title\": ct_title,\n",
    "                \"citing_pmid\": cpmid,\n",
    "                \"citation_sentence\": sent,\n",
    "                \"vader_score\": v_vader,\n",
    "                \"textblob_score\": v_tb,\n",
    "                \"custom_score\": v_custom,\n",
    "                \"pos_ratio\": r_pos,\n",
    "                \"neg_ratio\": r_neg,\n",
    "                \"mc_top1_label\": mc_label,\n",
    "                \"mc_top1_prob\": mc_prob,\n",
    "                \"mc_labels_multi\": ';'.join(mc_multi),\n",
    "                \"hf_sent_score\": sb_pos\n",
    "            })\n",
    "        time.sleep(PAUSE)\n",
    "    except Exception as e:\n",
    "        print(\"WARN:\", cpmid, e)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(rows, columns=[\n",
    "    \"source_title\",\"source_journal\",\"citing_title\",\"citing_pmid\",\"citation_sentence\",\n",
    "    \"vader_score\",\"textblob_score\",\"custom_score\",\"pos_ratio\",\"neg_ratio\",\n",
    "    \"mc_top1_label\",\"mc_top1_prob\",\"mc_labels_multi\",\"hf_sent_score\"\n",
    "])\n",
    "out_csv = os.path.join(OUT_DIR, f\"{TARGET_PMID}_citations_transformers.csv\")\n",
    "df.to_csv(out_csv, index=False)\n",
    "out_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f67ade-3172-4814-a197-7a9f50f4b05b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wholebif)",
   "language": "python",
   "name": "wholebif"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
